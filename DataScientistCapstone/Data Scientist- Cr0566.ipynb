{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ibmos2spark\n",
    "\n",
    "# @hidden_cell\n",
    "credentials = {\n",
    "    'endpoint': 'https://s3-api.us-geo.objectstorage.service.networklayer.com',\n",
    "    'api_key': '457ujvkZFpxzaSpTVd17JZcXboCkhgFD9oRYrmkNF-Bh',\n",
    "    'service_id': 'iam-ServiceId-f8a7d4db-3036-49fd-b9b5-db115af6ca48',\n",
    "    'iam_service_endpoint': 'https://iam.bluemix.net/oidc/token'}\n",
    "\n",
    "configuration_name = 'os_f68250162c014fe99aa26a4b35ce9144_configs'\n",
    "cos = ibmos2spark.CloudObjectStorage(sc, credentials, configuration_name, 'bluemix_cos')\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "import ibmos2spark\n",
    "# @hidden_cell\n",
    "credentials = {\n",
    "    'endpoint': 'https://s3-api.us-geo.objectstorage.service.networklayer.com',\n",
    "    'service_id': 'iam-ServiceId-dd47bdd1-03d0-4e17-8204-7f738de96ff4',\n",
    "    'iam_service_endpoint': 'https://iam.ng.bluemix.net/oidc/token',\n",
    "    'api_key': 'WHQii9cabgFDbT4mId7fqtmIxqAJ5MFypFzrCuWqFVHm'\n",
    "}\n",
    "\n",
    "configuration_name = 'os_4ed955c22f764c8e80316ae28dbb9b37_configs'\n",
    "cos = ibmos2spark.CloudObjectStorage(sc, credentials, configuration_name, 'bluemix_cos')\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "# Since JSON data can be semi-structured and contain additional metadata, it is possible that you might face issues with the DataFrame layout.\n",
    "# Please read the documentation of 'SparkSession.read()' to learn more about the possibilities to adjust the data loading.\n",
    "# PySpark documentation: http://spark.apache.org/docs/2.0.2/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader.json\n",
    "\n",
    "df_data_1 = spark.read.json(cos.url('medium-sparkify-event-data.json', 'datascientistsparkify-donotdelete-pr-bz8dxtt9csbrcm'))\n",
    "df_data_1.take(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg, col, concat, count, desc, explode, lit, min, max, split, stddev, udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import CountVectorizer, IDF, Normalizer, PCA, RegexTokenizer, StandardScaler, StopWordsRemover, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.mllib.tree import RandomForest, RandomForestModel\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.sql import functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"Spark - Capstone Project\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load, and Clean Dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_data_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_data_1\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for null values and empty strings\n",
    "#Check for empty strings in userId and sessionId\n",
    "print( df.where(col(\"userId\").isNull()).count() )\n",
    "print( df.where(col(\"sessionId\").isNull()).count() )\n",
    "print( df.where(col(\"userId\") == \"\").count() )\n",
    "print( df.where(col(\"sessionId\") == \"\").count() )\n",
    "df = df.filter(df.userId !=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check for other null values \n",
    "\n",
    "dataset_size = df.count()\n",
    "for c in df.columns:\n",
    "    null_count_c = df.where(col(c).isNull()).count()\n",
    "    proportion_null = null_count_c/dataset_size\n",
    "    print( c + ' : ' + str(null_count_c) + ' : ' + str(proportion_null))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Exploratory Data Analysis\n",
    "When you're working with the full dataset, perform EDA by loading a small subset of the data and doing basic manipulations within Spark. In this workspace, you are already provided a small subset of data you can explore.\n",
    "\n",
    "## Define Churn\n",
    "Once you've done some preliminary analysis, create a column Churn to use as the label for your model. I suggest using the Cancellation Confirmation events to define your churn, which happen for both paid and free users. As a bonus task, you can also look into the Downgrade events.\n",
    "\n",
    "## Explore Data\n",
    "Once you've defined churn, perform some exploratory data analysis to observe the behavior for users who stayed vs users who churned. You can start by exploring aggregates on these two groups of users, observing how much of a specific action they experienced per a certain time unit or number of songs played."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timespan of the data\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import udf, col, lit, min as Fmin, max as Fmax, sum as Fsum, expr\n",
    "from pyspark.sql.types import IntegerType, DateType, FloatType, DoubleType\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.classification import GBTClassifier, RandomForestClassifier, LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import datetime\n",
    "\n",
    "print( 'Min timestamp = ' + str( datetime.datetime.utcfromtimestamp( df.agg(Fmin(col(\"ts\"))).collect()[0][0]/1000  ).strftime('%c') ) )\n",
    "print( 'Max timestamp = ' + str( datetime.datetime.utcfromtimestamp( df.agg(Fmax(col(\"ts\"))).collect()[0][0]/1000  ).strftime('%c') ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.select(\"userId\").dropDuplicates().count())\n",
    "print(df.select(\"sessionId\").dropDuplicates().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distinct values taken by different categorical variables at the app level and their respective number of occurences in the data\n",
    "\n",
    "# The following fields are excluded here since they are user activity specific. serId, sessionId, artist\n",
    "\n",
    "categorical_variables = [ 'auth' , 'gender', 'level', 'method', 'page', 'status' ]\n",
    "\n",
    "for cvar in categorical_variables:\n",
    "    print(cvar)\n",
    "#     df.select(cvar).dropDuplicates().sort(cvar).show(100, False)\n",
    "    df.groupby(cvar).count().sort('count', ascending = False).show(100, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining churn\n",
    "#### We define churn as a user cancelling their service. If we see a \"Cancellation Confirmation\" event for a user, then we would consider that user to have churned\n",
    "\n",
    "#### Given our definition of churn, downgrades of service from paid to free tier could be potential indicators of churn too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Churn is a label for user who cancelled\n",
    "# Define a flag function\n",
    "from pyspark.sql import Window\n",
    "# Order dataframe by userId and timestamp\n",
    "df = df.orderBy(\"userId\", \"ts\")\n",
    "\n",
    "# Function to convert milliseconds to days\n",
    "days = lambda x: x * (1000 * 60 * 60 * 24)\n",
    "\n",
    "# Flag cancellation within the next 3 days\n",
    "churn_window = Window.partitionBy(df['userId']).orderBy(df['ts']).rangeBetween(0, days(3))\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"churn\", f.max(f.when(df.page == \"Cancellation Confirmation\", 1).otherwise(0)).over(churn_window)\n",
    ")\n",
    "\n",
    "\n",
    "# Collect userIds for users who churned\n",
    "users_that_churned = df.filter(df['churn'] == 1).select('userId').dropDuplicates().collect()\n",
    "users_that_churned_ids = [i.userId for i in users_that_churned]\n",
    "\n",
    "# Isolate page actions for users that churned\n",
    "churned = df.filter(df['userId'].isin(users_that_churned_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Again define pandas df for better view\n",
    "dfp = df.toPandas()\n",
    "\n",
    "dfp.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore Data\n",
    "Once you've defined churn, perform some exploratory data analysis to observe the behavior for users who stayed vs users who churned. You can start by exploring aggregates on these two groups of users, observing how much of a specific action they experienced per a certain time unit or number of songs played"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Know number of users who remain and who canceled\n",
    "# by pandas\n",
    "print(dfp.drop_duplicates(subset='userId').groupby(['churn'])['userId'].count())\n",
    "\n",
    "# Exploring user status by gender\n",
    "\n",
    "print(df.dropDuplicates(['userId', 'gender']).groupby(['churn', 'gender']).count().show())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring user status by paid lever\n",
    "\n",
    "print(df.dropDuplicates(['userId', 'level']).groupby(['churn', 'level']).count().show())\n",
    "\n",
    "def plot_frequency(subset, group, labels, x_title=\"Number of users\", y_title=\"Subscription status\"):\n",
    "    ax = dfp.drop_duplicates(subset=subset).groupby(\n",
    "                                group)['userId'].count().plot(\n",
    "                                kind='barh', figsize=(8,5), \n",
    "                                title='Number of unique users per category');\n",
    "    ax.set_xlabel(x_title);\n",
    "    ax.set_yticklabels(labels, rotation=0)\n",
    "    ax.set_ylabel(y_title);\n",
    "\n",
    "# Test    \n",
    "plot_frequency(['userId'], ['churn'], ['Active', 'Cancelled'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_frequency(['userId', 'gender'], \n",
    "               ['gender', 'churn'], \n",
    "               ['Active-Female', 'Cancelled-Female', 'Active-Male', 'Cancelled-Male'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_frequency(['userId', 'level'], \n",
    "               ['level', 'churn'], \n",
    "               ['Active-Free', 'Cancelled-Free', 'Active-Paid', 'Cancelled-Paid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "agents = [x.strip() for x  in dfp.userAgent.unique()] \n",
    "# see sample\n",
    "[agents[x] for x in [0, 2, 4, 10]]\n",
    "\n",
    "# TO get the data between parenthesis\n",
    "ex = '\\(([^\\)]*)\\)'\n",
    "\n",
    "# All OSs\n",
    "sorted(list(set([re.findall(\n",
    "    ex, x)[0].split(';')[0].capitalize(\n",
    ") for x  in agents])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mappers for the os\n",
    "mapper_general = {'Compatible': 'Windows',  'Ipad': 'Mac',  'Iphone': 'Mac',  \n",
    "          'Macintosh': 'Mac',  'Windows nt 5.1': 'Windows',  \n",
    "          'Windows nt 6.0': 'Windows',  'Windows nt 6.1': 'Windows',  \n",
    "          'Windows nt 6.2': 'Windows',  'Windows nt 6.3': 'Windows',  \n",
    "          'X11': 'Linux'}\n",
    "mapper_specific = {'Compatible': 'Windows 7',  'Ipad': 'iPad',  'Iphone': 'iPhone',  \n",
    "          'Macintosh': 'MacOS',  'Windows nt 5.1': 'Windows XP',  \n",
    "          'Windows nt 6.0': 'Windows Vista',  'Windows nt 6.1': 'Windows 7',  \n",
    "          'Windows nt 6.2': 'Windows 8.0',  'Windows nt 6.3': 'Windows 8.1',  \n",
    "          'X11': 'Linux'}\n",
    "print(sorted(list(set([mapper_general[re.findall(ex, x)[0].split(';')[0].capitalize()] for x  in agents]))),\n",
    "sorted(list(set([mapper_specific[re.findall(ex, x)[0].split(';')[0].capitalize()] for x  in agents]))))\n",
    "\n",
    "\n",
    "# Define user defined functions\n",
    "os_general = udf(lambda x: mapper_general[re.findall(ex, x)[0].split(';')[0].capitalize()])\n",
    "df = df.withColumn(\"os_general\", os_general(df.userAgent))\n",
    "\n",
    "os_specific = udf(lambda x: mapper_specific[re.findall(ex, x)[0].split(';')[0].capitalize()])\n",
    "df = df.withColumn(\"os_specific\", os_specific(df.userAgent))\n",
    "\n",
    "\n",
    "\n",
    "## Now we want to get a users states \n",
    "\n",
    "states = set([state[1].strip() for state in [x.split(',') for x in dfp.location.unique()]])\n",
    "# Define a user defined function\n",
    "get_state = udf(lambda x: x.split(',')[1].strip())\n",
    "df = df.withColumn(\"state\", get_state(df.location))\n",
    "\n",
    "# Convert to pandas for better visuals\n",
    "dfp = df.toPandas()\n",
    "dfp.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_series(col, churn, normalize=False):\n",
    "    ''' \n",
    "    Get a series either of churn users or existing users\n",
    "    THe function can normalize the series and return a percent value\n",
    "    col: {str} the column to plot (hour, month, day, week_day)\n",
    "    churn: {int} 0 or 1\n",
    "    normalize: {bool} T/F, Default is False\n",
    "    return a sorted pandas series\n",
    "    '''\n",
    "    ser1 = dfp[dfp.churn == churn].groupby([col])['userId'].count()\n",
    "    try:\n",
    "        ser1.index = ser1.index.astype(int)\n",
    "    except:\n",
    "        pass\n",
    "    if normalize:\n",
    "        ser1=ser1/ser1.sum()*100\n",
    "    return ser1.sort_index()\n",
    "\n",
    "def draw_time(col, normalize=True, figsize=(16,4), title=None, label_rotation=0):\n",
    "    '''\n",
    "    Draw a bar plot for churn and existing users based on \n",
    "         a specific column (hour, month, day, week_day)\n",
    "         \n",
    "    normalize: {bool} T/F, Default is True\n",
    "    figsize: {tuple} the plot size, default is (16,4)\n",
    "    title: descriptive title part\n",
    "    Returns: None\n",
    "    '''\n",
    "    df_time = pd.DataFrame({'Cancelled': get_series(col, 1, normalize),\n",
    "                            'Active users':get_series(col, 0, normalize)})\n",
    "    ax = df_time.plot(kind='bar', figsize=figsize);\n",
    "    ax.set_ylabel('Percent of users')\n",
    "    if title is None:\n",
    "        title = col\n",
    "    ax.set_ylabel(f'Percent of users')\n",
    "    #print(ax.get_xticks())\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=label_rotation)\n",
    "    ax.set_title(f'Percent of users took action per {title}') \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_time('os_general');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_time('os_specific');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_time('state', label_rotation=90);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "Initial EDA suggests maintaining the timeseries format of the data set may be our best tact. We will attempt to engineer features that lean in to this by collecting additional information from the timestamp and using rolling windows to summarize recent actions.\n",
    "\n",
    "Feature Engineering\n",
    "Once you've familiarized yourself with the data, build out the features you find promising to train your model on. To work with the full dataset, you can follow the following steps.\n",
    "\n",
    "\n",
    "Overview\n",
    "There are many featurs to keep, Categoric features (needs to be converted to numeric form through dummy encoding):\n",
    "- Gender of user\n",
    "- OS of user\n",
    "- State of residence that person lives in\n",
    "- Subscription level: paid or free\n",
    "- Popular artists with user\n",
    "\n",
    "\n",
    "Numeric features:\n",
    "- Song length per user per session\n",
    "- ThumbsUp/ThumpbsDown/InviteFriends/downgrades/songs per sissio/session duration/ length of subscription/number of days as free or paid user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create hour / day / month / year fields\n",
    "# Convert timestamp\n",
    "to_time_string = f.udf(lambda x: datetime.datetime.fromtimestamp(x / 1000.0).strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "df = df.withColumn('timestamp', f.to_timestamp(to_time_string(df['ts'])))\n",
    "\n",
    "# Roll up to datestamp\n",
    "df = df.withColumn('datestamp', df['timestamp'].cast('date'))\n",
    "\n",
    "\n",
    "df = df.withColumn(\"hour\", f.hour(\"timestamp\"))\n",
    "df = df.withColumn(\"day\", f.dayofmonth(\"timestamp\"))\n",
    "df = df.withColumn(\"month\", f.month(\"timestamp\"))\n",
    "df = df.withColumn(\"year\", f.year(\"timestamp\"))\n",
    "\n",
    "def days_since_reg(x, y):\n",
    "    \"\"\"\n",
    "    Retrieve rounded number of days since registration\n",
    "    \"\"\"\n",
    "    sec_diff = f.when(x > y, x - y).otherwise(0)    \n",
    "    return f.round(sec_diff / (1000 * 60 * 60 * 24))\n",
    "\n",
    "\n",
    "# Days since registration\n",
    "df = df.withColumn(\n",
    "    'days_since_reg', days_since_reg(df.ts, df.registration)\n",
    ")\n",
    "\n",
    "# Convert gender to binary column\n",
    "df = df.withColumn('gender_male', f.when(df.gender == 'M', 1).otherwise(0))\n",
    "\n",
    "# Convert subscription level to binary column\n",
    "df = df.withColumn('paid_level', f.when(df.level == 'paid', 1).otherwise(0))\n",
    "\n",
    "# List of unique page entries\n",
    "pages = [row.page for row in df.select(df.page).dropDuplicates().collect()]\n",
    "\n",
    "for page in pages:\n",
    "    \n",
    "    # Flag matching pages\n",
    "    page_col = page.replace(\" \", \"\")+\"_page\"\n",
    "    df = df.withColumn(page_col, f.when(df.page == page, 1).otherwise(0))\n",
    "    \n",
    "    # for 1, 7, and 30 day intervals...\n",
    "    for i in [1, 7, 30]:\n",
    "                \n",
    "        # Rolling window funciton for i days\n",
    "        w = (Window().partitionBy(df['userId']).orderBy(df['ts']).rangeBetween(-days(i), 0))\n",
    "\n",
    "        # Create column counting sum of page events in rolling window\n",
    "        window_col = page.replace(\" \", \"\") + \"_last{}d\".format(i)\n",
    "        df = df.withColumn(window_col, f.sum(page_col).over(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create features vector\n",
    "exclude_cols = ['artist', 'auth', 'firstName', 'gender', 'itemInSession', 'lastName', 'length',\n",
    "                'level', 'location', 'method', 'page', 'registration', 'sessionId', 'song', 'status',\n",
    "                'ts', 'userAgent', 'userId', 'timestamp', 'datestamp', 'id', 'churn', \n",
    "                'CancellationConfirmation_page', 'CancellationConfirmation_last1d', \n",
    "                'CancellationConfirmation_last7d', 'CancellationConfirmation_last30d', 'state', 'os_specific', 'os_general']\n",
    "feature_cols = [x for x in df.columns if x not in exclude_cols]\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"raw_features\")\n",
    "df = assembler.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# number of changing songs\n",
    "song_user_df = df.filter(df.page == 'NextSong').groupBy(\n",
    "                        'userId', 'sessionId').count()\n",
    "song_user_df = song_user_df.groupBy('userId').agg(\n",
    "                        sF.avg('count').alias('mean_songs'), \n",
    "                        sF.stddev('count').alias('stdev_songs'))\n",
    "song_user_df = song_user_df.na.fill(0)\n",
    "song_user_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of artists the user fans\n",
    "artists_user_fans = df.select('userId', 'artist').dropDuplicates().groupBy('userId').count().withColumnRenamed(\"count\", \"num_aritst\")\n",
    "artists_user_fans.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now am finding the session duration\n",
    "\n",
    "session_end = df.groupBy('userId', 'sessionId').max('ts').withColumnRenamed(\n",
    "                                                            'max(ts)', 'end')\n",
    "session_start = df.groupBy('userId', 'sessionId').min('ts').withColumnRenamed(\n",
    "                                                            'min(ts)', 'start')\n",
    "session_df = session_start.join(session_end,['userId', 'sessionId'])\n",
    "ticks_per_hours = 1000 * 60 * 60\n",
    "session_df = session_df.select('userId', 'sessionId', ((\n",
    "    session_df.end-session_df.start)/ticks_per_hours).alias('session_hours'))\n",
    "\n",
    "# 2-G.2  Get Average, and Standard deviation of the session duration per user\n",
    "\n",
    "session_user_df = session_df.groupBy('userId').agg(\n",
    "                        sF.avg('session_hours').alias('mean_session_h'), \n",
    "                        sF.stddev('session_hours').alias('stdev_session_h'))\n",
    "# Fill NaN with 0\n",
    "session_user_df = session_user_df.na.fill(0)\n",
    "session_user_df.show(3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Count sessions per user \n",
    "\n",
    "num_sessions_user_df = df.select('userId', 'sessionId').dropDuplicates().groupby('userId').count().withColumnRenamed('count', 'num_sessions')\n",
    "num_sessions_user_df.show(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of ThumbsUp, ThumbsDown, InviteFriends, downgrades, etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get the Number of ThumbsUp, ThumbsDown, InviteFriends, downgrades, ...\n",
    "# I think it is better to get all actions except Chorn actions (Cancel, cancelation confirmation)\n",
    "# The to normalize them as percent to sum all to 100.\n",
    "from functools import reduce\n",
    "# The distribution of pages per user (FILLING NAN with 0)\n",
    "user_page_distribution = df.groupby('userId').pivot('page').count().na.fill(0) #.toPandas().head(30)\n",
    "\n",
    "# Drop Cancel\tCancellation Confirmation columns\n",
    "user_page_distribution = user_page_distribution.drop(*['Cancel','Cancellation Confirmation'])\n",
    "\n",
    "# Normalizing each row to sum to 1\n",
    "\n",
    "# SOURCES\n",
    "# https://stackoverflow.com/questions/47641076/spark-normalize-each-row-of-a-dataframe\n",
    "# https://stackoverflow.com/questions/31955309/add-column-sum-as-new-column-in-pyspark-dataframe\n",
    "\n",
    "# the columns to be summed\n",
    "pages_cols = user_page_distribution.columns[1:]\n",
    "\n",
    "# Add a total column\n",
    "new_df = user_page_distribution.withColumn('total', sum(user_page_distribution[col] for col in pages_cols))\n",
    "\n",
    "# Apply normalization per column\n",
    "for col in pages_cols:\n",
    "    new_df = new_df.withColumn(f'norm_{col}', new_df[col] / new_df['total'] * 100.)\n",
    "    \n",
    "# Remove the total column    \n",
    "new_df = new_df.drop('total')\n",
    "\n",
    "# Remove the original columns\n",
    "new_df = new_df.drop(*pages_cols)\n",
    "\n",
    "# Rename the normalized columns back\n",
    "oldColumns = new_df.columns\n",
    "newColumns = ['userId'] + pages_cols\n",
    "user_page_distribution = reduce(lambda new_df, idx: new_df.withColumnRenamed(oldColumns[idx], newColumns[idx]), range(len(oldColumns)), new_df)\n",
    "\n",
    "# Freeup memory\n",
    "new_df=None\n",
    "\n",
    "user_page_distribution.toPandas().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## I want to get the length of a user's subscription \n",
    "\n",
    "def subLength(df, col = 'days_on'):\n",
    "    \"\"\"\n",
    "    Function gets timestamp and then merges into one dataframe\n",
    "    \"\"\"\n",
    "    reg_ts = df.select('userId', 'registration').dropDuplicates().withColumnRenamed('registration', 'start')\n",
    "    end_ts = df.groupBy('userId').max('ts').withColumnRenamed('max(ts)', 'end')\n",
    "    reg_df = reg_ts.join(end_ts,'userId')\n",
    "    ticks_per_day = 1000 * 60 * 60 * 24 \n",
    "    reg_df = reg_df.select('userId', ((reg_df.end-reg_df.start)/ticks_per_day).alias(col))\n",
    "    return reg_df\n",
    "\n",
    "reg_df = subLength(df, col='days_total_subscription')\n",
    "reg_df.show(10)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## I'm like to see the number of days of each user id as a free or a paid user \n",
    "\n",
    "paid_free_df = df.filter(df.page=='NextSong').groupBy('userId').pivot('level').count()\n",
    "paid_free_df = paid_free_df.na.fill(0)\n",
    "active_cols = paid_free_df.columns[1:]\n",
    "paid_free_df = paid_free_df.withColumn('total', \n",
    "                                       sum(paid_free_df[col] for col in active_cols))\n",
    "for col in active_cols:\n",
    "    paid_free_df = paid_free_df.withColumn(f'{col}_percent', \n",
    "                                           paid_free_df[col] / paid_free_df.total * 100)\n",
    "active_cols.append('total')    \n",
    "paid_free_df = paid_free_df.drop(*active_cols)\n",
    "paid_free_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Isolate feature and label columns\n",
    "data = df.select(df[\"churn\"].alias(\"label\"), df[\"raw_features\"])\n",
    "\n",
    "# Split code for training / validation\n",
    "train, test = data.randomSplit([0.7, 0.3], seed=420)\n",
    "\n",
    "# Instantiate standard scaler\n",
    "scaler = StandardScaler(inputCol=\"raw_features\", outputCol=\"features\", withStd=True)\n",
    "\n",
    "# Instantiate classifier\n",
    "gbc = GBTClassifier(lossType='logistic', seed=420)\n",
    "\n",
    "pipeline = Pipeline(stages=[scaler, gbc])\n",
    "\n",
    "\n",
    "#def features_join(new, old):\n",
    "#    df_joined = old\n",
    "#    new = new.join(df_joined, 'userId', how = 'inner')\n",
    "#    return new.dropDuplicates()\n",
    "#\n",
    "#for i, feature in enumerate(user_features):\n",
    "#    final_df = features_join(final_df, feature)\n",
    "#\n",
    "#final_df = final_df.orderBy('userId', ascending=True)\n",
    "#final_df = final_df.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfinal_df = final_df.toPandas()\n",
    "pfinal_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Creating, Refinement, and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(gbc.maxDepth, [2, 4, 6])\n",
    "             .addGrid(gbc.maxBins, [20, 60])\n",
    "             .addGrid(gbc.maxIter, [10, 20])\n",
    "             .build())\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=3)\n",
    "\n",
    "cvModel = crossval.fit(train)\n",
    "predictions = cvModel.transform(test)\n",
    "evaluator.evaluate(predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect label and probability values\n",
    "preds_list = preds.select('label','probability')\\\n",
    "    .rdd.map(lambda row: (float(row['probability'][1]), float(row['label'])))\\\n",
    "    .collect()\n",
    "\n",
    "# Pass through roc_curve function\n",
    "y_score, y_true = zip(*preds_list)\n",
    "y_score_rev = [1 - x for x in y_score]\n",
    "fpr, tpr, thresholds = roc_curve(y_true, y_score_rev, pos_label = 1)\n",
    "\n",
    "# Plot ROC Curve\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12,7))\n",
    "ax.plot(tpr, fpr)\n",
    "\n",
    "# Format chart\n",
    "ax.set_ylabel(\"True Positive Rate\")\n",
    "ax.set_xlabel(\"False Positive Rate\")\n",
    "ax.set_title(\"ROC Curve\", fontsize=14)\n",
    "\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert predictions to a dataframe\n",
    "preds_df = pd.DataFrame({'preds': y_score, 'label': y_true})\n",
    "\n",
    "# Iterate over possible thresholds and capture metrics for each\n",
    "thresholds = [x / 100.0 for x in list(range(1,99))]\n",
    "accuracy = []\n",
    "recall = []\n",
    "precision = []\n",
    "f1_score = []\n",
    "\n",
    "for t in thresholds:\n",
    "    tp = len(preds_df.query(f\"label == 1 and preds >= {t}\"))\n",
    "    fp = len(preds_df.query(f\"label == 0 and preds >= {t}\"))\n",
    "    tn = len(preds_df.query(f\"label == 0 and preds < {t}\"))\n",
    "    fn = len(preds_df.query(f\"label == 1 and preds < {t}\"))\n",
    "    \n",
    "    acc = (tp+tn)/(tp+fp+tn+fn)\n",
    "    rec = tp / (tp+fn)\n",
    "\n",
    "    try:\n",
    "        prec = tp / (tp+fp)\n",
    "    except ZeroDivisionError:\n",
    "        prec = 0\n",
    "    \n",
    "    try:\n",
    "        f1 = (prec * rec) / (prec + rec)\n",
    "    except:\n",
    "        f1 = 0\n",
    "    \n",
    "    accuracy.append(acc)\n",
    "    recall.append(rec)\n",
    "    precision.append(prec)\n",
    "    f1_score.append(f1)\n",
    "    \n",
    "# Plot Accuracy / Recall / Precision\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12,8))\n",
    "ax.plot(thresholds, accuracy, label='Accuracy')\n",
    "ax.plot(thresholds, recall, label='Recall')\n",
    "ax.plot(thresholds, specificity, label=\"Specificity\")\n",
    "ax.plot(thresholds, precision, label='Precision')\n",
    "ax.plot(thresholds, f1_score, label=\"F1 Score\")\n",
    "\n",
    "# Format chart\n",
    "ax.set_ylabel(\"Performance Rate\")\n",
    "ax.set_xlabel(\"Threshold\")\n",
    "ax.set_title(\"Performance by Threshold\", fontsize=14)\n",
    "\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reflection \n",
    "\n",
    "- Overall I think that next steps could be getting into data streaming with spark. Businesses are fueled off quick responses, so incorporating data streaming is vital for a company's success.\n",
    "- I could work with more observations by increaseing data set size\n",
    "- I could work on incorporating more parameters or going else where to get more data on the user.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# References\n",
    "Tausend, F. “Hands-on: Predict Customer Churn.” Medium, Towards Data Science, 1 June 2019, towardsdatascience.com/hands-on-predict-customer-churn-5c2a42806266.\n",
    "Patel, Neil. “How to Improve Your Subscription Based Business by Predicting Churn.” Neil Patel, 24 May 2019, neilpatel.com/blog/improve-by-predicting-churn/.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 with Spark",
   "language": "python3",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
